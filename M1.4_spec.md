# Milestone 1.4 Specification: RSS Source Seeding and Single-Feed Ingestion

## Overview

**Goal:** Seed initial RSS sources and implement a minimal ingestion pipeline to fetch, parse, and persist items from a single feed into the database.

**Duration:** 1–2 days

**Dependencies:**

- Milestone 1.2 (FastAPI Skeleton) completed
- Milestone 1.3 (Database Bootstrap) completed and migrations applied

## Objectives

Establish the first end-to-end ingestion slice:

- Seed 3 high-quality RSS sources into the `sources` table
- Implement a simple ingestion module using `feedparser` for one feed
- Provide a CLI command to ingest a single feed on demand
- Persist raw fields into `articles` with baseline metadata
- Basic structured logging and minimal error handling

## Technical Requirements

### 1. Dependencies

- Add `feedparser` for RSS parsing
- Optional: `python-dateutil` for robust datetime parsing
- Add `httpx` for HTTP fetching with explicit timeouts and headers
- Add `bleach` for HTML sanitization to prevent XSS
- Add `pydantic` for input validation

```txt
# backend/requirements.txt additions
feedparser==6.0.11
python-dateutil==2.9.0.post0
httpx==0.27.2
bleach==6.1.0  # HTML sanitization for XSS prevention
pydantic>=2.0.0  # Input validation
```

### 2. Project Structure

Add a minimal ingestion module and a seed file:

```
backend/
├── app/
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── feed_client.py     # Fetch via httpx (UA/timeout) + feedparser.parse(bytes)
│   │   ├── mapper.py          # Map feed entries → Article fields
│   │   └── ingest_one.py      # CLI entrypoint for single-feed ingestion
│   ├── data/
│   │   └── sources.seed.json  # Initial set of 3 sources
│   └── core/
│       └── config.py          # Add minimal ingestion settings (UA, timeout)
└── tests/
    ├── test_ingestion.py
    └── fixtures/
        └── feeds/
            └── sample_feed.xml
```

### 3. Source Seeding

Seed an initial set of 3 reputable technology feeds. The seed script performs idempotent upserts into `sources` on `feed_url`.

**Security Requirements:**

- Use SQLAlchemy ORM with parameterized queries to prevent SQL injection
- Validate all input data using Pydantic models
- Never construct SQL queries using string concatenation or f-strings

```json
// backend/app/data/sources.seed.json (example)
[
  {
    "name": "TechCrunch",
    "url": "https://techcrunch.com",
    "feed_url": "https://techcrunch.com/feed/",
    "credibility_score": 0.7,
    "is_active": true
  },
  {
    "name": "The Verge",
    "url": "https://www.theverge.com",
    "feed_url": "https://www.theverge.com/rss/index.xml",
    "credibility_score": 0.7,
    "is_active": true
  },
  {
    "name": "BBC Technology",
    "url": "https://www.bbc.com/news/technology",
    "feed_url": "http://feeds.bbci.co.uk/news/technology/rss.xml",
    "credibility_score": 0.7,
    "is_active": true
  }
]
```

CLI to seed sources:

```bash
conda run -n ymb-py311 python -m app.ingestion.ingest_one --seed-sources app/data/sources.seed.json
```

Behavior:

- Validate JSON schema: `name`, `url`, `feed_url` (required); optional `credibility_score`, `is_active`
- Upsert by `feed_url`; update mutable fields on conflict
- Log counts: created, updated, skipped

### 4. Ingestion: Single Feed

Implement a CLI that ingests a single feed by URL (or by `source_id`).

**Security Requirements:**

- Implement SSRF protection by blocking private IP ranges and localhost
- Always verify SSL/TLS certificates when fetching feeds
- Sanitize all HTML content using `bleach` with a strict allowlist (empty by default)
- Validate URLs before making HTTP requests
- Limit response size to prevent DoS (max 10MB)

```bash
conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url https://techcrunch.com/feed/
# or
conda run -n ymb-py311 python -m app.ingestion.ingest_one --source-id 1
```

Core steps:

1. Resolve `Source` by `feed_url` or `source_id` using parameterized queries; create inactive placeholder if missing
2. Validate and sanitize the feed URL to prevent SSRF attacks:
   - Block private IP ranges (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 127.0.0.0/8)
   - Only allow HTTP/HTTPS protocols
   - Verify SSL certificates (no self-signed)
3. Fetch the feed bytes via `httpx` using configured `INGESTION_USER_AGENT` and `INGESTION_TIMEOUT_SEC`; then parse with `feedparser.parse(response.content)`
4. Map entries to `Article` fields, storing raw description into `summary_raw`
5. Compute `content_hash` for dedup compatibility with schema
6. Persist new `Article` rows; skip gracefully on unique-constraint collisions
7. Update `Source.last_fetched_at`; increment `error_count` and `last_error` on failures
8. Emit structured logs and a final summary (fetched, parsed, inserted, skipped, errors)

Field mapping (typical sources; tolerate missing fields):

- `title`: Sanitize `entry.title` to remove any HTML/JavaScript
- `link`: Validate and canonicalize `entry.link`; fallback to first `links[rel="alternate"]`
- `summary_raw`: Sanitize `entry.summary` or `entry.description` using bleach:
  ```python
  import bleach
  # Strip ALL HTML tags, no exceptions
  cleaned = bleach.clean(entry.summary, tags=[], strip=True)
  # Trim to 4000 chars to prevent oversized rows
  summary_raw = cleaned[:4000]
  ```
- `published_at`: parse `entry.published` or `entry.updated`; normalize to UTC. If both missing, fallback to feed-level timestamp when available; else use a deterministic fallback (midnight UTC on current date) and mark as fallback in logs.
- `author`: `entry.author` (optional)
- `tags`: `[t.term for t in entry.tags]` if present, else empty list

`content_hash` (baseline; will be refined in M1.5):

```text
sha256(lowercase(strip(title)) + "|" + canonical_link + "|" + (iso8601(published_at_utc) OR "fallback:" + first_8_chars_of_sha256(link)))
```

Notes:

- Use canonicalized link (normalized URL without tracking params) for hashing stability
- When a published time fallback is used, include "fallback:" prefix plus first 8 chars of link hash instead of a timestamp. This maintains determinism while significantly reducing collision risk for articles without published dates
- Example: If published_at is missing, use `"fallback:a3f2b8c1"` where `a3f2b8c1` is from `sha256(canonical_link)[:8]`

Error handling:

- Network errors: retry with exponential backoff and jitter. Defaults: attempts=3; `sleep = base * (2^k) + random(0, jitter)`, where `base=0.5s`, `jitter=0.3s`; cap total wait per feed to 8s
- Parse errors: log and continue to next entry
- DB unique constraint (`content_hash`) collisions: count as `skipped`

### 5. Configuration

Add minimal ingestion settings to `app/core/config.py`:

- `INGESTION_USER_AGENT` (string; default `YourMorningBriefBot/0.1 (+contact: dev@local)`)
- `INGESTION_TIMEOUT_SEC` (int; default 10)
- `INGESTION_MAX_RETRIES` (int; default 2)
- `RETRY_BACKOFF_BASE_SEC` (float; default 0.5)
- `RETRY_BACKOFF_JITTER_SEC` (float; default 0.3)
- `INGESTION_TOTAL_RETRY_CAP_SEC` (float; default 8.0)
- `SUMMARY_MAX_LEN` (int; default 4000)
- `MAX_RESPONSE_SIZE_MB` (int; default 10)
- `BLOCKED_NETWORKS` (list; default private IP ranges)
- `ALLOWED_URL_SCHEMES` (list; default ['http', 'https'])

These are read by `feed_client.py` for HTTP requests and by the CLI for behavior.

### 6. Makefile Targets

Add helper targets for repeatable local runs:

```makefile
.PHONY: seed-sources ingest-one

seed-sources:
	@echo "Seeding sources from JSON..."
	conda run -n ymb-py311 python -m app.ingestion.ingest_one --seed-sources app/data/sources.seed.json

ingest-one:
	@echo "Ingesting single feed..."
	@if [ -z "$(FEED_URL)" ]; then echo "Provide FEED_URL=..."; exit 1; fi
	conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url "$(FEED_URL)"
```

### 7. Logging and Metrics

Use existing structured logging foundation (from M1.2) and add:

- Per-run summary: `{"feed_url": ..., "fetched": N, "inserted": N, "skipped": N, "errors": N, "duration_ms": X}`
- Per-error logs capture exception types and brief messages; no sensitive data

### 8. Testing Requirements

#### Unit Tests

- Mapping correctness: feed entries → `Article` fields
- `content_hash` stability across runs given same inputs
- Datetime parsing to UTC with various input formats
- Graceful handling of missing/optional fields
- **Security Tests:**
  - XSS sanitization removes all HTML/JavaScript
  - SQL injection prevention with malicious input
  - SSRF protection blocks private IP ranges
  - URL validation rejects non-HTTP(S) schemes

#### Integration Tests

- Use a local fixture XML feed (`tests/fixtures/feeds/sample_feed.xml`) served from disk; mock `feedparser.parse` to avoid network
- Seed a `Source`, run CLI ingestion, assert:
  - ≥10 `Article` rows created (given a sufficiently large fixture)
  - `summary_raw` populated when available
  - `published_at` timezone-aware and in UTC
  - Re-running against same feed yields `skipped` due to unique constraint on `content_hash`

#### CLI Tests

- Argument parsing: `--seed-sources`, `--feed-url`, `--source-id` mutually exclusive where appropriate
- Exit codes: non-zero on unrecoverable failures

### 9. Docker/Compose (Optional for M1.4)

- No new long-running services required. For convenience, document how to exec into the backend container and run the CLI.

```bash
docker compose exec backend bash -lc "python -m app.ingestion.ingest_one --feed-url https://techcrunch.com/feed/"
```

Note: Inside the container, use system Python (no Conda). On the host, use Conda as shown in prior commands.

## Quality Gates

### Code Quality

- `ruff` linting passes
- `black` formatting applied
- `mypy` type checking passes in `app/ingestion/`

### Performance

- Single-feed ingestion completes in < 5s on a typical broadband connection (dev env)
- Parser handles at least 50 items without timeouts

### Security

- Respect configured timeouts
- Do not log full HTML content; summaries only
- User-Agent configurable via settings

## Acceptance Criteria

1. **Source Seeding:**
   - Running `seed-sources` creates or updates 3 sources
   - `sources` table reflects `name`, `url`, `feed_url`, and defaults

2. **Single-Feed Ingestion:**
   - CLI command ingests ≥10 items from one feed. For deterministic validation, the integration test uses the local fixture `tests/fixtures/feeds/sample_feed.xml` containing ≥10 entries (no network dependency)
   - Inserted rows include: `source_id`, `title`, `link`, `summary_raw` (if provided), `published_at`, `content_hash`
   - Re-running on the same feed does not create duplicates (skips due to unique `content_hash`)

3. **Observability:**
   - Structured logs show per-run summary and key events
   - Errors are logged without crashing the process

## Deliverables

1. **Source Seed:** `backend/app/data/sources.seed.json`
2. **Ingestion Module:** `backend/app/ingestion/` with `feed_client.py`, `mapper.py`, `ingest_one.py`
3. **Makefile Targets:** `seed-sources`, `ingest-one`
4. **Tests:** `tests/test_ingestion.py` and fixtures
5. **Documentation:** README updates covering seeding and ingestion usage
6. **Preconditions:** Explicit note that `sources.feed_url` and `articles.content_hash` are unique (from M1.3), which this milestone relies upon for idempotency

## Success Metrics

- ✅ Seeded 3 sources visible in DB
- ✅ Single-feed CLI inserts ≥10 `Article` records on first run
- ✅ Subsequent run produces 0 new inserts and reports `skipped` duplicates
- ✅ All CI checks pass (lint, type-check, tests)

## Next Steps

- **Milestone 1.5:** Implement normalization and robust deduplication; enforce stable `content_hash` generation with improved heuristics
- **Milestone 1.6:** Expand to 10+ feeds, add per-source backoff/retry and metrics
- **Milestone 2.5:** Schedule ingestion to run periodically
