# Milestone 1.5 Specification: Normalization, Storage, and Deduplication

## Overview

**Goal:** Normalize incoming feed items into a consistent schema and enforce idempotent storage by robust deduplication using a stable `content_hash` and database constraints.

**Duration:** 2 days

**Dependencies:**

- Milestone 1.4 (RSS Source Seeding and Single-Feed Ingestion) completed
- Database tables and unique constraint on `articles.content_hash` present from M1.3

## Objectives

- Normalize article fields (title, link, summary, published_at, author, tags)
- Compute a stable `content_hash` using normalized values
- Make ingestion idempotent: re-ingesting the same feed yields 0 new rows
- Add unit and integration tests to validate normalization and dedup behavior

## Technical Requirements

### 1. Dependencies

Add character encoding detection and security libraries for robust feed handling:

```txt
# backend/requirements.txt additions
chardet==5.2.0  # or charset-normalizer for character encoding detection
bleach==6.1.0  # HTML sanitization (if not already added in M1.4)
```

### 2. Project Structure

Add a normalization and dedup pipeline integrated with the ingestion flow:

```
backend/
├── app/
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── feed_client.py
│   │   ├── mapper.py
│   │   └── ingest_one.py
│   ├── processing/
│   │   ├── __init__.py
│   │   ├── encoding.py         # Character encoding detection/normalization
│   │   ├── normalization.py    # Clean/normalize parsed entries
│   │   └── content_hash.py     # Stable hashing strategy
│   └── core/
│       └── config.py           # Add normalization toggles
└── tests/
    ├── test_encoding.py
    ├── test_normalization.py
    ├── test_deduplication.py
    └── fixtures/
        └── feeds/
            ├── near_duplicate_samples.xml
            └── encoding_samples/   # Various encoding test files
```

### 3. Normalization Rules

Apply the following deterministic rules before persistence:

- **Character Encoding Security:**
  - Whitelist allowed encodings: UTF-8, UTF-16, ISO-8859-1, Windows-1252, ASCII
  - Validate detected encoding confidence (minimum 80%)
  - Test decode before accepting encoding
  - Fall back to UTF-8 with 'replace' error handling for safety
  - Limit sample size for encoding detection to 10KB to prevent DoS
- **Unicode:** after encoding normalization, apply Unicode normalization (NFKC) to all textual fields prior to downstream processing
- **Title:** trim whitespace, collapse internal whitespace, strip HTML tags/entities deterministically, lowercase only for hashing (store original-cased but use normalized form for hash)
- **Link (canonicalization):**
  - Lowercase scheme/host; punycode domains; remove default ports
  - Remove fragments; remove trailing slash unless content-significant
  - Sort query params; remove tracking params (from `LINK_TRACKING_PARAMS`)
  - Resolve relative URLs to absolute when possible
- **Summary (`summary_raw`):**
  - Strip ALL HTML tags using bleach with empty allowlist: `bleach.clean(text, tags=[], strip=True)`
  - Remove any JavaScript or script content
  - Decode HTML entities safely
  - Collapse whitespace
  - Limit to `SUMMARY_MAX_LEN` (default 4k chars) to avoid huge payloads
  - Log any suspicious content patterns for security monitoring
- **Published Time:** parse various formats; default to `updated` if `published` missing; convert to timezone-aware UTC; if missing, use feed-level `updated_parsed` or current UTC as last resort (flagged)
- **Author:** trim; allow None
- **Tags:** deduplicate case-insensitively, sort for stable ordering

### 4. Stable Content Hash

Compute `content_hash` using normalized values to maximize stability across trivial variations:

```text
sha256(
  lowercase(trimmed_title) + "|" +
  canonical_link + "|" +
  iso8601(published_at_utc) + "|" +
  first_100_chars_of(cleaned_summary)
)
```

Notes:

- Use lowercase for title only in the hash; store original for display
- Canonical link must be normalized as above
- If `published_at` missing, include fallback timestamp used to avoid collisions
- Summary contribution reduces duplicates across same title/link days, but remains stable across whitespace/HTML differences
- Consider a `HASH_ALGO_VERSION` constant to future-proof changes to normalization/hash inputs (document-only in M1.5; no migration required yet)

### 5. Ingestion Integration

- Extend `mapper.py` to call `processing.normalization.normalize_entry(...)`
- **Security validation:** Validate all normalized content before persistence
- Compute `content_hash` via `processing.content_hash.compute(...)` using sanitized inputs only
- On attempted insert, use parameterized queries via SQLAlchemy ORM
- Catch unique constraint violations (same `content_hash`) and count as `skipped`
- Add `--normalize/--no-normalize` flag to `ingest_one.py`; default enabled

Determinism guardrails:

- Ensure all normalization uses the same code path across CLI runs and tests
- Document that cross-source duplicates will deduplicate when title/link/published_at align after normalization; `source_id` is not part of the hash

CLI examples:

```bash
conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url https://techcrunch.com/feed/
conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url https://techcrunch.com/feed/ --no-normalize
```

### 6. Configuration

Extend `app/core/config.py`:

- `NORMALIZE_ENABLED` (bool, default True)
- `HASH_SUMMARY_PREFIX_LEN` (int, default 100)
- `LINK_TRACKING_PARAMS` (list[str], default `["utm_source","utm_medium","utm_campaign","utm_term","utm_content","fbclid","gclid","mc_cid","mc_eid"]`)
- `SUMMARY_MAX_LEN` (int, default 4000)
- `ALLOWED_ENCODINGS` (list[str], default `["utf-8","utf-16","iso-8859-1","windows-1252","ascii"]`)
- `ENCODING_CONFIDENCE_MIN` (float, default 0.8)
- `ENCODING_SAMPLE_SIZE` (int, default 10240)

### 7. Makefile Targets

```makefile
.PHONY: ingest-one-normalized

ingest-one-normalized:
	@echo "Ingesting with normalization..."
	@if [ -z "$(FEED_URL)" ]; then echo "Provide FEED_URL=..."; exit 1; fi
	conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url "$(FEED_URL)"
```

### 8. Logging

- Per-article debug log includes whether normalization modified any field (counts only, not full text)
- Per-run summary includes `normalized_modified_count`

## Testing Requirements

### Unit Tests

- Character encoding detection and normalization (test with various encodings: UTF-8, ISO-8859-1, Windows-1252)
- **Security Tests:**
  - Encoding validation rejects disallowed encodings
  - XSS payloads are completely removed from content
  - HTML injection attempts are sanitized
  - Malformed encoding attacks are handled safely
- Title/link/summary normalization correctness and stability
- Removal of tracking parameters in links
- Timezone normalization to UTC
- Tag deduplication and sorting
- `content_hash` reproducibility across runs

### Integration Tests

- Given a fixture with near-duplicate entries (minor punctuation/HTML differences), only one `Article` is inserted
- Re-running ingestion yields 0 new inserts (idempotent)
- `summary_raw` is trimmed and sanitized

### Property-Based Tests (Optional)

- Small mutations (extra whitespace, HTML tags, order of tags) do not change `content_hash`

## Quality Gates

### Code Quality

- `ruff` and `black` pass
- `mypy` passes for `app/processing/` and ingestion modules

### Performance

- Normalization pipeline adds < 50ms overhead per 50 entries locally

### Security

- Never log full summary text (truncate to 200 chars max)
- Sanitize ALL HTML content using bleach with empty allowlist
- Validate encoding detection results against whitelist
- Use parameterized queries for all database operations
- Monitor for suspicious content patterns and log security events

## Acceptance Criteria

1. Re-ingestion of the same feed results in 0 new inserts (idempotent)
2. Normalized fields stored according to rules; `content_hash` stable
3. Dedup enforced by DB unique index and application logic
4. Tests cover normalization and dedup with realistic fixtures

## Deliverables

1. `app/processing/normalization.py` and `content_hash.py`
2. Updated ingestion flow invoking normalization and hashing
3. Tests: `tests/test_normalization.py`, `tests/test_deduplication.py`
4. Makefile target `ingest-one-normalized`
5. README updates for normalization behavior

## Success Metrics

- ✅ First ingestion inserts N items; second run inserts 0
- ✅ Near-duplicate fixtures insert only 1 record
- ✅ All CI checks pass

## Next Steps

- **Milestone 1.6:** Scale ingestion to 10–15 feeds with robust error handling
- **Milestone 2.2:** Ranking will depend on normalized fields for consistency
