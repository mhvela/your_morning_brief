# Milestone 1.5 Specification: Normalization, Storage, and Deduplication

## Overview

**Goal:** Normalize incoming feed items into a consistent schema and enforce idempotent storage by robust deduplication using a stable `content_hash` and database constraints.

**Duration:** 2 days

**Dependencies:**

- Milestone 1.4 (RSS Source Seeding and Single-Feed Ingestion) completed
- Database tables and unique constraint on `articles.content_hash` present from M1.3

## Objectives

- Normalize article fields (title, link, summary, published_at, author, tags)
- Compute a stable `content_hash` using normalized values
- Make ingestion idempotent: re-ingesting the same feed yields 0 new rows
- Add unit and integration tests to validate normalization and dedup behavior

## Technical Requirements

### 1. Dependencies

Add character encoding detection and security libraries for robust feed handling:

```txt
# backend/requirements.txt additions
chardet==5.2.0  # or charset-normalizer for character encoding detection
bleach==6.1.0  # HTML sanitization (if not already added in M1.4)
```

### 2. Project Structure

Add a normalization and dedup pipeline integrated with the ingestion flow:

```
backend/
├── app/
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── feed_client.py
│   │   ├── mapper.py
│   │   └── ingest_one.py
│   ├── processing/
│   │   ├── __init__.py
│   │   ├── encoding.py         # Character encoding detection/normalization
│   │   ├── normalization.py    # Clean/normalize parsed entries
│   │   └── content_hash.py     # Stable hashing strategy
│   └── core/
│       └── config.py           # Add normalization toggles
└── tests/
    ├── test_encoding.py
    ├── test_normalization.py
    ├── test_deduplication.py
    └── fixtures/
        └── feeds/
            ├── near_duplicate_samples.xml
            └── encoding_samples/   # Various encoding test files
```

### 3. Normalization Rules

Apply the following deterministic rules before persistence:

- **Character Encoding Security:**
  - Whitelist allowed encodings: UTF-8, UTF-16, ISO-8859-1, Windows-1252, ASCII
  - Validate detected encoding confidence (minimum 80%)
  - Test decode before accepting encoding
  - Fall back to UTF-8 with 'replace' error handling for safety
  - Limit sample size for encoding detection to 10KB to prevent DoS
- **Unicode:** after encoding normalization, apply Unicode normalization (NFKC) to all textual fields prior to downstream processing
- **Title:** trim whitespace, collapse internal whitespace, strip HTML tags/entities deterministically, lowercase only for hashing (store original-cased but use normalized form for hash)
- **Link (canonicalization):**
  - Lowercase scheme/host; punycode domains; remove default ports
  - Remove fragments; remove trailing slash unless content-significant
  - Sort query params; remove tracking params (from `LINK_TRACKING_PARAMS`)
  - Resolve relative URLs to absolute when possible
- **Summary (`summary_raw`):**
  - Strip ALL HTML tags using bleach with empty allowlist: `bleach.clean(text, tags=[], strip=True)`
  - Remove any JavaScript or script content
  - Decode HTML entities safely
  - Collapse whitespace
  - Limit to `SUMMARY_MAX_LEN` (default 4k chars) to avoid huge payloads
  - Log any suspicious content patterns for security monitoring
- **Published Time:** parse various formats; default to `updated` if `published` missing; convert to timezone-aware UTC; if missing, use feed-level `updated_parsed` or current UTC as last resort (flagged)
- **Author:** trim; allow None
- **Tags:** deduplicate case-insensitively, sort for stable ordering

### 4. Stable Content Hash

Compute `content_hash` using normalized values to maximize stability across trivial variations:

```text
sha256(
  lowercase(trimmed_title) + "|" +
  canonical_link + "|" +
  iso8601(published_at_utc) + "|" +
  first_100_chars_of(cleaned_summary)
)
```

Notes:

- Use lowercase for title only in the hash; store original for display
- Canonical link must be normalized as above
- If `published_at` missing, include fallback timestamp used to avoid collisions
- Summary contribution reduces duplicates across same title/link days, but remains stable across whitespace/HTML differences
- Consider a `HASH_ALGO_VERSION` constant to future-proof changes to normalization/hash inputs (document-only in M1.5; no migration required yet)

### 5. Ingestion Integration

- Extend `mapper.py` to call `processing.normalization.normalize_entry(...)`
- **Security validation:** Validate all normalized content before persistence
- Compute `content_hash` via `processing.content_hash.compute(...)` using sanitized inputs only
- On attempted insert, use parameterized queries via SQLAlchemy ORM
- Catch unique constraint violations (same `content_hash`) and count as `skipped`
- Add `--normalize/--no-normalize` flag to `ingest_one.py`; default enabled

Determinism guardrails:

- Ensure all normalization uses the same code path across CLI runs and tests
- Document that cross-source duplicates will deduplicate when title/link/published_at align after normalization; `source_id` is not part of the hash

CLI examples:

```bash
conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url https://techcrunch.com/feed/
conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url https://techcrunch.com/feed/ --no-normalize
```

### 6. Configuration

Extend `app/core/config.py`:

- `NORMALIZE_ENABLED` (bool, default True)
- `HASH_SUMMARY_PREFIX_LEN` (int, default 100)
- `LINK_TRACKING_PARAMS` (list[str], default `["utm_source","utm_medium","utm_campaign","utm_term","utm_content","fbclid","gclid","mc_cid","mc_eid"]`)
- `SUMMARY_MAX_LEN` (int, default 4000)
- `ALLOWED_ENCODINGS` (list[str], default `["utf-8","utf-16","iso-8859-1","windows-1252","ascii"]`)
- `ENCODING_CONFIDENCE_MIN` (float, default 0.8)
- `ENCODING_SAMPLE_SIZE` (int, default 10240)

### 7. Makefile Targets

```makefile
.PHONY: ingest-one-normalized

ingest-one-normalized:
	@echo "Ingesting with normalization..."
	@if [ -z "$(FEED_URL)" ]; then echo "Provide FEED_URL=..."; exit 1; fi
	conda run -n ymb-py311 python -m app.ingestion.ingest_one --feed-url "$(FEED_URL)"
```

### 8. Logging

- Per-article debug log includes whether normalization modified any field (counts only, not full text)
- Per-run summary includes `normalized_modified_count`

## Testing Requirements

### Unit Tests

- Character encoding detection and normalization (test with various encodings: UTF-8, ISO-8859-1, Windows-1252)
- **Security Tests:**
  - Encoding validation rejects disallowed encodings
  - XSS payloads are completely removed from content
  - HTML injection attempts are sanitized
  - Malformed encoding attacks are handled safely
- Title/link/summary normalization correctness and stability
- Removal of tracking parameters in links
- Timezone normalization to UTC
- Tag deduplication and sorting
- `content_hash` reproducibility across runs

### Integration Tests

- Given a fixture with near-duplicate entries (minor punctuation/HTML differences), only one `Article` is inserted
- Re-running ingestion yields 0 new inserts (idempotent)
- `summary_raw` is trimmed and sanitized

### Property-Based Tests (Optional)

- Small mutations (extra whitespace, HTML tags, order of tags) do not change `content_hash`

## Quality Gates

### Code Quality

- `ruff` and `black` pass
- `mypy` passes for `app/processing/` and ingestion modules

### Performance

- Normalization pipeline adds < 50ms overhead per 50 entries locally

### Security

- Never log full summary text (truncate to 200 chars max)
- Sanitize ALL HTML content using bleach with empty allowlist
- Validate encoding detection results against whitelist
- Use parameterized queries for all database operations
- Monitor for suspicious content patterns and log security events

## Acceptance Criteria

1. Re-ingestion of the same feed results in 0 new inserts (idempotent)
2. Normalized fields stored according to rules; `content_hash` stable
3. Dedup enforced by DB unique index and application logic
4. Tests cover normalization and dedup with realistic fixtures

## Deliverables

1. `app/processing/normalization.py` and `content_hash.py`
2. Updated ingestion flow invoking normalization and hashing
3. Tests: `tests/test_normalization.py`, `tests/test_deduplication.py`
4. Makefile target `ingest-one-normalized`
5. README updates for normalization behavior

## Success Metrics

- ✅ First ingestion inserts N items; second run inserts 0
- ✅ Near-duplicate fixtures insert only 1 record
- ✅ All CI checks pass

## TDD Implementation Plan

### Phase 1: Test Foundation & Structure Setup

#### 1.1 Create Test Fixtures and Structure

- Create `backend/tests/fixtures/feeds/` directory with test RSS feeds:
  - `near_duplicate_samples.xml` - feeds with minor variations (whitespace, HTML differences)
  - `encoding_samples/` - feeds in UTF-8, ISO-8859-1, Windows-1252 for encoding tests
- Set up `backend/tests/test_encoding.py`, `backend/tests/test_normalization.py`, `backend/tests/test_deduplication.py`

#### 1.2 Write Failing Tests First (TDD Red Phase)

- **Encoding Tests**: Character detection, validation, security constraints
- **Normalization Tests**: Title/link/summary/tags normalization rules
- **Content Hash Tests**: Stable hash generation across variations
- **Integration Tests**: End-to-end idempotent ingestion
- **Security Tests**: XSS sanitization, encoding validation, malicious content handling

### Phase 2: Core Processing Module Implementation

#### 2.1 Create Processing Package Structure

```
backend/app/processing/
├── __init__.py
├── encoding.py         # Character encoding detection/normalization
├── normalization.py    # Clean/normalize parsed entries
├── content_hash.py     # Stable hashing strategy
```

#### 2.2 Implement Character Encoding Security (TDD Green Phase)

- `encoding.py`: Safe encoding detection with whitelisting
- Security validations: confidence thresholds, sample size limits
- Comprehensive fallback handling with UTF-8 'replace' error mode

#### 2.3 Implement Normalization Pipeline (TDD Green Phase)

- `normalization.py`: Deterministic field normalization
- Title: Unicode NFKC, whitespace collapse, HTML stripping
- Link canonicalization: tracking param removal, URL normalization
- Summary: HTML sanitization with empty allowlist, length limits
- Author/tags: sanitization and deduplication

#### 2.4 Implement Stable Content Hashing (TDD Green Phase)

- `content_hash.py`: SHA256-based stable hashing
- Hash inputs: lowercase title + canonical link + UTC timestamp + summary prefix
- Fallback handling for missing timestamps

### Phase 3: Configuration Enhancement

#### 3.1 Extend Configuration (TDD Green Phase)

Add to `app/core/config.py`:

- `NORMALIZE_ENABLED` (bool, default True)
- `HASH_SUMMARY_PREFIX_LEN` (int, default 100)
- `LINK_TRACKING_PARAMS` (list[str])
- `SUMMARY_MAX_LEN` (int, default 4000)
- `ALLOWED_ENCODINGS`, `ENCODING_CONFIDENCE_MIN`, `ENCODING_SAMPLE_SIZE`

### Phase 4: Integration with Existing Ingestion

#### 4.1 Enhance Mapper Integration (TDD Green Phase)

- Modify `app/ingestion/mapper.py` to call normalization pipeline
- Replace existing sanitization with comprehensive normalization
- Integrate stable content hash generation
- Add validation for normalized content before persistence

#### 4.2 Update CLI Tools (TDD Green Phase)

- Add `--normalize/--no-normalize` flag to `ingest_one.py`
- Implement normalization toggle functionality
- Add logging for normalization modifications

### Phase 5: Database Integration & Deduplication

#### 5.1 Enhance Database Operations (TDD Green Phase)

- Implement proper unique constraint violation handling
- Add `skipped` counter for duplicate detection
- Ensure parameterized queries via SQLAlchemy ORM

#### 5.2 Idempotent Storage Logic (TDD Green Phase)

- Handle duplicate content hash insertions gracefully
- Track and report deduplication statistics
- Ensure stable behavior across multiple ingestion runs

### Phase 6: Makefile & CLI Enhancement

#### 6.1 Add Makefile Targets (TDD Green Phase)

- Implement `ingest-one-normalized` target
- Ensure proper error handling and parameter validation

### Phase 7: Testing & Quality Assurance

#### 7.1 Comprehensive Test Coverage (TDD Refactor Phase)

- Unit tests for all normalization functions
- Property-based tests for hash stability
- Integration tests for idempotent ingestion
- Security tests for malicious content handling
- Performance tests for normalization overhead

#### 7.2 Quality Gates Validation

- Ensure `ruff`, `black`, `mypy` compliance
- Validate performance requirements (<50ms overhead per 50 entries)
- Security validation (no full text logging, proper sanitization)

### Phase 8: Documentation & Completion

#### 8.1 Update Documentation

- README updates for normalization behavior
- Code documentation for new modules
- Migration notes for configuration changes

#### 8.2 Milestone Validation

- Verify all acceptance criteria met
- Test idempotent behavior with real RSS feeds
- Validate deduplication with fixture samples
- Ensure all CI checks pass

### Key TDD Principles Applied

1. **Red-Green-Refactor Cycle**: Write failing tests first, implement minimal code to pass, then refactor
2. **Test-First Design**: Let tests drive the API design of new modules
3. **Comprehensive Coverage**: Unit, integration, and security tests
4. **Incremental Development**: Build functionality piece by piece with tests guiding implementation
5. **Quality Assurance**: Continuous validation of type safety, security, and performance requirements

## Next Steps

- **Milestone 1.6:** Scale ingestion to 10–15 feeds with robust error handling
- **Milestone 2.2:** Ranking will depend on normalized fields for consistency
