# Milestone 1.6 Specification: Expand to 10+ Feeds and Basic Error Handling

## Overview

**Goal:** Scale ingestion to 10–15 sources with resilient fetching, per-source backoff/retry, and basic source health metrics without crashing on individual feed failures.

**Duration:** 2 days

**Dependencies:**

- Milestone 1.4 (Single-Feed Ingestion)
- Milestone 1.5 (Normalization & Dedup)

## Objectives

- Support batch ingestion across multiple active sources
- Implement per-source retry and exponential backoff on transient failures
- Track source-level status and metrics (success/failure counts, last error)
- Ensure failures are isolated; one failing feed does not stop the run

## Technical Requirements

### 1. Project Structure

Add batch ingestion orchestrator and metrics:

```
backend/
├── app/
│   ├── ingestion/
│   │   ├── ingest_batch.py    # Iterate active sources with per-source resilience
│   │   └── metrics.py         # Source-level counters and summaries
│   └── core/
│       └── config.py          # Backoff/retry settings
└── tests/
    └── test_batch_ingestion.py
```

### 2. Batch Ingestion

- Query `sources` where `is_active=True` using parameterized queries
- Order sources by fairness: `ORDER BY last_fetched_at NULLS FIRST, id ASC`
- For each source:
  - **Security validation:** Validate feed URL against SSRF blocklist before fetching
  - Attempt fetch/parse/normalize/insert using existing single-feed pipeline
  - Enforce SSL/TLS certificate verification for all HTTPS feeds
  - On network errors, retry with exponential backoff; cap total attempts
  - On parse errors, skip entry; on persistent feed-level parse error, mark source `last_error`
  - Update `last_fetched_at` when successful; increment `error_count` otherwise
  - **Rate limiting:** Respect per-source rate limits to prevent abuse
- Aggregate per-run metrics and emit a final summary log

CLI:

```bash
conda run -n ymb-py311 python -m app.ingestion.ingest_batch --limit 15
```

Flags:

- `--limit` (int): maximum number of sources to ingest this run (default 10)
- `--only-feed-url` or `--only-source-id` to target subset
- `--parallel N` (optional, default 1) to process sources concurrently (simple thread pool); ensure DB session safety

### 3. Backoff/Retry Policy & Security Configuration

Configuration-driven defaults:

- `RETRY_MAX_ATTEMPTS` (default 3)
- `RETRY_BACKOFF_BASE_SEC` (default 1.0)
- `RETRY_BACKOFF_JITTER_SEC` (default 0.3)
- `MIN_FETCH_INTERVAL_SEC` (default 60) to avoid hammering recently-fetched sources
- `AUTO_DEACTIVATE_AFTER_ERRORS` (optional, default 0 disabled): when >0, auto-set `is_active=false` after this many consecutive failures
- Database connection pool settings (see Concurrency section)

**Security Settings:**

- `SSL_VERIFY` (bool, default True) - Always verify SSL certificates
- `MAX_REDIRECTS` (int, default 3) - Limit redirects to prevent redirect loops
- `REQUEST_TIMEOUT` (int, default 10) - Timeout for each request
- `MAX_RESPONSE_SIZE_MB` (int, default 10) - Prevent DoS via large responses
- `BLOCKED_IP_RANGES` (list) - Private/local IP ranges to block for SSRF protection

Formula per attempt k (0-indexed): `sleep = base * (2^k) + uniform(0, jitter)`

Transient error classes:

- Connection errors, timeouts, 5xx HTTP responses

Non-retryable errors:

- Obvious permanent errors (e.g., 404 on feed URL), malformed URL; mark source with `last_error`

### 4. Source Health and Metrics

Persist on `Source`:

- `last_fetched_at: datetime`
- `last_error: str | None`
- `error_count: int`

Per-run summary (structured log):

```json
{"sources_total": T, "sources_succeeded": S, "sources_failed": F, "articles_inserted": N, "articles_skipped": K, "duration_ms": D}
```

Optional metrics endpoints are deferred; logging suffices for this milestone.

Per-source summary log at INFO level after each source:

```json
{"source_id": S, "feed_url": "...", "inserted": N, "skipped": K, "errors": E, "duration_ms": D}
```

### 5. Makefile Targets

```makefile
.PHONY: ingest-batch

ingest-batch:
	@echo "Batch ingesting active sources..."
	conda run -n ymb-py311 python -m app.ingestion.ingest_batch --limit 15
```

### 6. Error Isolation

- Ensure one source failure does not abort the entire run
- Log errors at warning level; continue with next source

Concurrency & session/client reuse:

- Configure SQLAlchemy engine with appropriate connection pooling:
  ```python
  # In database configuration
  engine = create_engine(
      DATABASE_URL,
      pool_size=5,           # Number of persistent connections
      max_overflow=10,       # Maximum overflow connections
      pool_timeout=30,       # Timeout for getting connection from pool
      pool_recycle=3600,     # Recycle connections after 1 hour
      pool_pre_ping=True     # Test connections before using
  )
  ```
- Create one SQLAlchemy `Session` per worker/thread from the shared engine's connection pool; never share sessions across threads
- Each worker gets connections from the shared pool rather than creating new engines
- **Secure HTTP Client Configuration:**

  ```python
  import httpx
  import ipaddress

  # Per-worker secure client
  client = httpx.Client(
      timeout=httpx.Timeout(10.0),
      verify=True,  # Always verify SSL
      follow_redirects=True,
      max_redirects=3,
      limits=httpx.Limits(
          max_connections=10,
          max_keepalive_connections=5
      ),
      headers={
          'User-Agent': INGESTION_USER_AGENT,
          'Accept': 'application/rss+xml, application/xml, text/xml',
          'Connection': 'close'
      }
  )

  # SSRF protection function
  def is_private_ip(hostname: str) -> bool:
      try:
          ip = ipaddress.ip_address(hostname)
          return ip.is_private or ip.is_loopback or ip.is_link_local
      except ValueError:
          return False  # Not an IP, hostname OK
  ```

- Respect `MIN_FETCH_INTERVAL_SEC` between fetches for the same source
- Monitor pool statistics in logs when `DEBUG` level is enabled (active connections, pool overflow)

## Testing Requirements

### Unit Tests

- Backoff function produces increasing delays with jitter
- Retry policy respects max attempts and error classifications
- **Security Tests:**
  - SSRF protection blocks private IP ranges
  - SSL verification rejects self-signed certificates
  - Response size limits prevent memory exhaustion
  - Redirect limits prevent infinite loops

### Integration Tests

- Simulate mixed success/failure across multiple mock sources
- Verify metrics summary reflects counts correctly
- Verify `last_error` and `error_count` update as expected
- Confirm that successful sources still ingest when others fail
- Mock network and time to deterministically validate retry, backoff, and fairness ordering

### Concurrency (If `--parallel` used)

- Thread-safe DB sessions per worker; no cross-thread session reuse
- Verify no duplicate inserts across concurrent runs due to DB unique constraints

## Quality Gates

### Code Quality

- `ruff`/`black`/`mypy` pass

### Performance

- Batch of 10 feeds completes in < 20s under normal network conditions

### Reliability & Security

- Intermittent failures are retried; hard failures recorded without crashing
- SSL/TLS verification enforced for all HTTPS connections
- SSRF protection prevents access to internal network resources
- Response size limits prevent DoS attacks

## Acceptance Criteria

1. Ingest 10–15 sources in a run with per-source resilience
2. Failures do not crash the process; logs show clear summaries
3. Source health fields updated appropriately
4. Tests cover backoff, retry, and batch behavior

## Deliverables

1. `app/ingestion/ingest_batch.py` and `metrics.py`
2. Configured retry/backoff settings in `config.py`
3. Makefile target `ingest-batch`
4. Tests for batch ingestion and retry

## Success Metrics

- ✅ Batch run completes with clear summary
- ✅ Successful sources ingest while failing ones are isolated
- ✅ All CI checks pass

## Next Steps

- **Milestone 1.7:** Implement baseline relevance scoring per topic
- **Milestone 2.5:** Integrate scheduler to run batch ingestion periodically
